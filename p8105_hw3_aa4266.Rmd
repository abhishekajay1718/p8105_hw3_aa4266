---
title: "P8105 HW3"
author: "Abhishek Ajay (aa4266)"
date: "October 14, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.width = 8.5,
                      fig.asp = 0.9,
                      out.width = "90%"
                     )
library(p8105.datasets)
library(tidyverse)
library(ggplot2)
theme_set(theme_bw() + theme(legend.position = "bottom"))
```

#Problem 1

Data import and cleaning

```{r p1_data_import}
brfss_data_tidy = 
  p8105.datasets::brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(resp_id = respid, location_abbr = locationabbr, location_desc = locationdesc) %>% #further manual renaming to appropriate variable names
  filter(topic == "Overall Health") %>% 
  select(year, location_abbr, location_desc, response, data_value)

brfss_data_tidy$response = 
  brfss_data_tidy$response %>%  
  factor(levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))

brfss_data_tidy
```
Here we cleaned the data by focussing only on the "Overall  Health" topic. Under this topic the response are exhaustively "Excellent", "Very good", "Good", "Fair" and "Poor". 

We recoded the reponse column into a factor data type and set the levels from the "Excellent" to "Poor".

Now we do some *exploratory analysis* of the data in the following:

Q1. In 2002, which states were observed at 7 locations?

```{r p1.1_brfss}
brfss_data_tidy %>% 
  ungroup() %>% 
  filter(year == 2002) %>% 
  distinct(location_abbr, location_desc) %>% 
  group_by(location_abbr) %>% 
  count() %>% 
  filter(n == 7) %>% 
  select(location_abbr) %>% 
  knitr::kable()
```

In 2002, the states: CT, FL and NC were observed at 7 locations.

Q2. Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

```{r p1.2_brfss}
#Restructuring the data for the plot required and Spaghetting plot using ggplot
brfss_data_tidy %>% 
  ungroup() %>% 
  group_by(year, location_abbr) %>% 
  distinct(location_abbr, location_desc) %>% 
  summarise(location_count = n()) %>% 
  ggplot(aes(x = year, y = location_count, color = location_abbr)) +
    geom_line() +
    labs(
      title = "Number of locations in each state",
      x = "Year",
      y = "Number of locations",
      color = 'States'
    ) 
```

In the above we show a spaghetting plot for number of location sin each state from 2002 to 2010.

Q3. Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

```{r p1.3_brfss}
brfss_data_tidy %>% 
  ungroup() %>% 
  filter(year == 2002 | year == 2006 | year == 2010, location_abbr == "NY", response == "Excellent") %>% 
  group_by(year) %>%
  summarise(mean_excellent = mean(data_value, na.rm = TRUE),
            sd_excellent = sd(data_value, na.rm = TRUE)
  ) %>% 
  knitr::kable(digits = 2)
```

The above table displays the mean and standard deviation of the *excellent* responses from different locations in NY State for years 2002, 2006 and 2010

Q4. For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r p1.4_brfss}
brfss_data_tidy %>% 
  ungroup() %>% 
  group_by(year, location_abbr, response) %>% 
  summarise(mean_response = mean(data_value)) %>% 
  ggplot(aes(x = year, y = mean_response, color = location_abbr)) +
  geom_point() +
  facet_grid(~ response) +
  labs(
      title = "Distribution of state-level averages for each year",
      x = "Year",
      y = "Average Proportion (%)"
    ) 
```

In the above five-panel plot, we show the distribution of the proportion of the five responses per year per state.


#Problem 2

Here we work on the Instacart data from the p8105.datasets package.

Data Import
```{r p2_data_import}
instacart_data =
  p8105.datasets::instacart
#summary(instacart_data) %>% 
 # View()
```
###Data Description

The instacart dataset has the dimensions (rows, columns): *`r dim(instacart_data)`*. It basically describes the various characteristics associated with a specific order, pin pointed using the order_id. There are *`r count(as.tibble(unique(x = instacart_data$order_id)))`* number of unique orders. There are *`r count(as.tibble(unique(x = instacart_data$user_id)))`* unique users. Since both the numbers are same, we infer that the each unique user had only one unique order (characterised by an order_id) by them. 

Some other Key variables apart from the obvious "order_id" and "user_id" as discussed above are the 

  * *`r count(as.tibble(unique(x = instacart_data$department_id)))`* unique departments, with each department given a unique "department_id", 
  
  * *`r count(as.tibble(unique(x = instacart_data$aisle_id)))`* aisles with unique "aisle_id", 
  
  * *`r count(as.tibble(unique(x = instacart_data$product_id)))`* unique products with each "product_name" with a unique "product_id".

We notice that "eval_set" for all orders in **train** and hence consider that column to be redundant since it can clearly be mentioned in single line in the data description.