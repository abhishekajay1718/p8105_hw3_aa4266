---
title: "P8105 HW3"
author: "Abhishek Ajay (aa4266)"
date: "October 14, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.width = 8.5,
                      fig.asp = 0.9,
                      out.width = "90%"
                     )
library(p8105.datasets)
library(tidyverse)
library(ggplot2)
library(hexbin)
theme_set(theme_bw() + theme(legend.position = "bottom"))
library(patchwork)
```

#Problem 1

Data import and cleaning

```{r p1_data_import}
brfss_data_tidy = 
  p8105.datasets::brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(resp_id = respid, location_abbr = locationabbr, location_desc = locationdesc) %>% #further manual renaming to appropriate variable names
  filter(topic == "Overall Health") %>% 
  select(year, location_abbr, location_desc, response, data_value)

brfss_data_tidy$response = 
  brfss_data_tidy$response %>%  
  factor(levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))

brfss_data_tidy
```
Here we cleaned the data by focussing only on the "Overall  Health" topic. Under this topic the response are exhaustively "Excellent", "Very good", "Good", "Fair" and "Poor". 

We recoded the reponse column into a factor data type and set the levels from the "Excellent" to "Poor".

Now we do some *exploratory analysis* of the data in the following:

Q1. In 2002, which states were observed at 7 locations?

```{r p1.1_brfss}
brfss_data_tidy %>% 
  ungroup() %>% 
  filter(year == 2002) %>% 
  distinct(location_abbr, location_desc) %>% 
  group_by(location_abbr) %>% 
  count() %>% 
  filter(n == 7) %>% 
  select(location_abbr) %>% 
  knitr::kable()
```

In 2002, the states: CT, FL and NC were observed at 7 locations.

Q2. Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

```{r p1.2_brfss}
#Restructuring the data for the plot required and Spaghetting plot using ggplot
brfss_data_tidy %>% 
  ungroup() %>% 
  group_by(year, location_abbr) %>% 
  distinct(location_abbr, location_desc) %>% 
  summarise(location_count = n()) %>% 
  ggplot(aes(x = year, y = location_count, color = location_abbr)) +
    geom_line() +
    labs(
      title = "Number of locations in each state",
      x = "Year",
      y = "Number of locations",
      color = 'States'
    ) 
```

In the above we show a spaghetting plot for number of location sin each state from 2002 to 2010.

Q3. Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

```{r p1.3_brfss}
brfss_data_tidy %>% 
  ungroup() %>% 
  filter(year == 2002 | year == 2006 | year == 2010, location_abbr == "NY", response == "Excellent") %>% 
  group_by(year) %>%
  summarise(mean_excellent = mean(data_value, na.rm = TRUE),
            sd_excellent = sd(data_value, na.rm = TRUE)
  ) %>% 
  knitr::kable(digits = 2)
```

The above table displays the mean and standard deviation of the *excellent* responses from different locations in NY State for years 2002, 2006 and 2010

Q4. For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r p1.4_brfss}
brfss_data_tidy %>% 
  ungroup() %>% 
  group_by(year, location_abbr, response) %>% 
  summarise(mean_response = mean(data_value)) %>% 
  ggplot(aes(x = year, y = mean_response, color = location_abbr)) +
  geom_point() +
  facet_grid(~ response) +
  labs(
      title = "Distribution of state-level averages for each year",
      x = "Year",
      y = "Average Proportion (%)"
    ) 
```

In the above five-panel plot, we show the distribution of the proportion of the five responses per year per state.


#Problem 2

Here we work on the Instacart data from the p8105.datasets package.

Data Import
```{r p2_data_import}
instacart_data =
  p8105.datasets::instacart
#summary(instacart_data) %>% 
 # View()

n_order_per_hour_of_day = 
  instacart_data %>% 
  group_by(order_hour_of_day) %>%
  distinct(order_id) %>% 
  summarise(n_order_per_hour = n()) %>% 
  arrange(desc(n_order_per_hour))

```
###Data Description

The instacart dataset has the dimensions (rows, columns): *`r dim(instacart_data)`*. It basically describes the various characteristics associated with a specific order, pin pointed using the order_id. There are *`r count(as.tibble(unique(x = instacart_data$order_id)))`* number of unique orders. There are *`r count(as.tibble(unique(x = instacart_data$user_id)))`* unique users. Since both the numbers are same, we infer that the each unique user had only one unique order (characterised by an order_id) by them. 

The structure of the dataset is: `r str(instacart_data)`

Some other Key variables apart from the obvious "order_id" and "user_id" as discussed above are the 

  * *`r nrow(instacart_data[unique(instacart_data$department_id),])`* unique departments, with each department given a unique "department_id", 
  
  * *`r nrow(instacart_data[unique(instacart_data$aisle_id),])`* aisles with unique "aisle_id", 
  
  * *`r nrow(instacart_data[unique(instacart_data$product_id),])`* unique products with each "product_name" with a unique "product_id".

We notice that "eval_set" for all orders in **train** and hence consider that column to be redundant since it can clearly be mentioned in single line in the data description.

Largest number of orders are done at the *`r head(n_order_per_hour_of_day$order_hour_of_day, 1)`* with *`r head(n_order_per_hour_of_day$n_order_per_hour, 1)`* orders. 

Least number of orders are taken at the *`r tail(n_order_per_hour_of_day$order_hour_of_day, 1)`* with *`r tail(n_order_per_hour_of_day$n_order_per_hour, 1)`* orders only.

Further exploratory analysis of the *"instacart_data"*. 

Q1. How many aisles are there, and which aisles are the most items ordered from?

```{r p2.1_instacart}
instacart_data %>% 
  group_by(aisle_id, aisle) %>% 
  select(order_id, aisle_id) %>% 
  summarise(n_orders_per_aisle = n()) %>% 
  arrange(desc(n_orders_per_aisle)) %>% 
  head(1) %>% 
  knitr::kable()
```
There are `r nrow(instacart_data[unique(instacart_data$aisle_id),])` aisles. The table was grouped with aisle and aisle_id just to make sure that the aisle name is mentioned in the table, elsewise, each aisle has a unique aisle_id and hence just taking either one of them for grouping won't change the answer. 

Since each order_id is unique to a single order, summarising along the group gives us the number of orders per aisle.

Q2. Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

```{r p2.2_instacart}
instacart_data %>% 
  group_by(aisle) %>% 
  summarise(n_items = n()) %>% 
  mutate(aisle = fct_reorder(aisle, desc(n_items))) %>% 
  ggplot(aes(x = aisle, y = n_items)) +
  geom_point() +
  scale_y_continuous(breaks = c(100, 1000, 10000, 100000, 150000),
                     limits = c(100, 160000), 
                     labels = c("100", "1000", "10000", "100000", "150000")
                     ) +
  theme(axis.text.x = element_text(color = "#993333", angle = 90, size = rel(0.8), hjust = 1)) +
  labs(
    title = "Number of items bought from each aisle",
    x = "Aisles",
    y = "Number of items", 
    caption = "Data from Instacart dataset"
  ) #From http://www.sthda.com/english/wiki/ggplot2-axis-ticks-a-guide-to-customize-tick-marks-and-label
```

We see that the fresh vegetables and the fresh fruits are the aisles with the most orders. We have used a continuous scale on the y-axis to show the drastic difference between the number of items bought from the fresh fruits and fresh vegetables aisles and the rest of the aisles. We see that the remaining aisles at the right end of the graph have more or less a similar amount of items sold through it. 

Q3. Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r p2.3_instacart}
instacart_data %>% 
  group_by(aisle, product_name) %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>%  
  summarise(pop_product = n()) %>% 
  group_by(aisle) %>% 
  filter(pop_product == max(pop_product)) %>% 
  knitr::kable(col.names = c("Aisle", "Product", "Amount Sold"))
```

The most popular items sold among each of the three aisles is calculated by first grouping aisles and products to get the count of each product sold per aisle and then by aisle to see the product sold the most in all of that given aisle.

Most popular products in the following aisles are: 

  * baking ingredients - Light Brown Sugar
  
  * dog food care - Snacks Sticks Chicken and Reci Recipe Dog Treats
  
  * packaged vegetables fruits - Organic Baby Spinach
  
Q4. Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r p2.4_instacart}
instacart_data %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  mutate(order_dow = recode(order_dow, '0' = "Sunday", '1' = "Monday", '2' = "Tuesday", '3' = "Wednesday", '4' = "Thursday", '5' = "Friday", '6' = "Saturday")) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(mean_hour_of_day = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour_of_day) %>% 
  knitr::kable(digits = 2)
```

Here we have a tablw showing the mean of the hour at which Coffee Ice Cream and Pink Lady Apples are ordered on a given day of week respectively.

We infer that the fruit -- Pink Lady Apples are ordered usually in the first half of the day while COffee Ice Cream is more prefered post lunch hours.

#Problem 3

Data import and exploration
```{r p3}
ny_noaa_data = 
  p8105.datasets::ny_noaa %>% 
  janitor::clean_names()

summary(ny_noaa_data) %>% 
  knitr::kable()
```

###Data Description

Here we have a data set from the *National Oceanic and Atmospheric Administration* taken from the p8105.datasets package that acquired the dataset from the rnoaa package avaialble on CRAN. 

It contains relevant weather informations collected from all New York state weather stations from January 1, 1981 through December 31, 2010. 

The dimensions of the dataset (rows, columns) are `r dim(ny_noaa_data)`. The key variables here are the weather station ID (column name: id), snowfall in mm (column name: snow), precipitation in tenths of mm (column name: prcp), min and max temperature in tenths of celsius (column names: tmin and tmax respectively).

The structure of the dataset is: `r str(ny_noaa_data)`

There are `r ny_noaa_data %>% filter(is.na(prcp)) %>% count()/nrow(ny_noaa_data) * 100`% NAs in the precipitation column. 

There are `r ny_noaa_data %>% filter(is.na(snow)) %>% count()/nrow(ny_noaa_data) * 100`% NA's in the snow column.

There are `r ny_noaa_data %>% filter(is.na(snwd)) %>% count()/nrow(ny_noaa_data) * 100`% NA's in the Snow depth (snwd) column.

There are `r ny_noaa_data %>% filter(is.na(tmin)) %>% count()/nrow(ny_noaa_data) * 100`% NA's in the minimum temperature column.

There are `r ny_noaa_data %>% filter(is.na(tmax)) %>% count()/nrow(ny_noaa_data) * 100`% NA's in the max temperature column.

As can be seen above, the issue of missing data is a real problem with our data set here since around 44% of our tmin and tmax values are missing, nearly 15% of our snowfall data is missing and henceforth. This is will result in lack of evidence to support or reject any hypothesis. 



Q1. Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?

```{r p3.1_noaa, cache = TRUE}

ny_noaa_data_tidy = 
  ny_noaa_data %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(month = as.integer(month), year = as.integer(year), day = as.integer(day), tmin = as.integer(tmin), tmax = as.integer(tmax)) %>% 
  mutate(month = month.name[month], tmin = tmin / 10, tmax = tmax / 10, prcp = prcp / 10) #since they're in tenths of mm/celsius in ny_noaa_data 
  
ny_noaa_most_snow =
  ny_noaa_data_tidy %>% 
  group_by(snow) %>% 
  count(snow) %>% 
  arrange(desc(n)) %>% 
  head()
```
We divide the precipitation, minimun temperature and maximum temperature by 10 since all these values are in tenths of their units (mm, degree celsius, degree celsius respectively) in the noaa dataset.

 `r knitr::kable(ny_noaa_most_snow, caption = "The most observed snowfall values over these 30 years", col.names = c("snowfall (mm)", "no. of observations"))`

Furthermore, the above table shows the 6 most observed values of precipitation in the noaa table. One can notice that 2nd most repeated observation is NA (i.e missing) and hence missing values are a serious problem in our data set due to its predominance in our dataset. 0 mm snowfall in the most observed value in all our dataset. This is because most of the time in the 30 years that we took station data, most days didn't have any snowfall. 

Q2. Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r p3.2_noaa}
ny_noaa_data_tidy %>% 
  group_by(year, month, id) %>% 
  filter(month == "January" | month == "July") %>% 
  summarise(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_tmax, group = year)) +
  geom_boxplot() +
  facet_grid(~ month) +
  labs(
    title = "Average max temperature for Jan and July in each station across 1981 to 2010",
    x = "Year",
    y = "Average max temperature (C)"
  )
```

There is a clear observable structure between the two panels, one from January and the other from July. We see that the average temperature in January has had a median near 0 degree celsius all through the years 1981 to 2010. However, the median of the average temperature in July has been somewhere between 25 and 20 degree celsius for the month of July all through the years. 

We do notice quite a few outliers. January 1994 had a median near -5 degree celsius, however, quite a few outliers are there near and aboe 0 degree celsius for that year. Similar scenario is observed for January 2009. It is helpful to notice that there are outlier is most of the years for January data except 1981, 1985, 1989, 1997 and 2006.

Similarly for the July datasets except for 1999, 2009 and 2010 all other years have outlier(s). However, most of the outliers are similarly located around 20 degree celsius. However, one station in 1987 had a noticably lower temperature than the rest with around 14 degree celsius.

Q3. Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r p3.3.1_noaa}
tmax_vs_tmin_p = 
  ny_noaa_data_tidy %>% 
  ggplot((aes(x = tmin, y = tmax))) +
  geom_hex(na.rm = TRUE) +
  labs(
    tile = "Max temperature vs Min temperature", 
    x = "Minimum temperature (C)",
    y = "Maximum temperature (C)",
    caption = "For the full dataset"
  ) +
  theme(legend.text = element_text(size = 9, hjust = 1, angle = 90))

snowfall_vs_year_p = 
  ny_noaa_data_tidy %>% 
  filter(snow > 0, snow < 100) %>% 
  ggplot(aes(x = year, y = snow, group = year)) +
  geom_boxplot(na.rm = TRUE) +
  labs(
    title = "Snowfall distribution (0,100) for each year",
    x = "Year",
    y = "Snowfall (mm)",
    caption = "Plotted for each year separately"
  )

tmax_vs_tmin_p + snowfall_vs_year_p
```
For the first plot we use a hex plot because we have an extremely large dataset and a scatterplot won't have any human readable output. However, the gradient in the hex plot allows for a better understanding of the continuous data we are dealing with here, i.e *tmax and tmin*.

We use boxplots for the second for a better understanding of the distribution of the snowfall amount over the years. It's clearly observable that the snowfall between 0 and 100 mm has had a more or less same median all through the year 1981 to 2010 at around 25 mm. We can also notic outliers such as that in 1998 with very high snowfall. 
